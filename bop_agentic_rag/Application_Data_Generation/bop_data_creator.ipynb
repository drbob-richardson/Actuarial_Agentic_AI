{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vnvE6LuhE0c",
        "outputId": "83f5c592-a7ad-4d3d-e344-0dfa321a8a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pymupdf, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 pymupdf-1.26.0 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain openai faiss-cpu pymupdf langchain-community tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "import os\n"
      ],
      "metadata": {
        "id": "SoHL2a4NhF6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup environment and API key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "P6ipCGk6hQnf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDF\n",
        "pdf_path = \"bop_policyguide_draft2.pdf\"  # adjust path if needed\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "#  Split into retrievable chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "# Tag each chunk with its source\n",
        "for chunk in chunks:\n",
        "    chunk.metadata[\"source\"] = \"bop_guidelines_draft2.pdf\"\n",
        "\n",
        "# Create embeddings using OpenAI\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create and save FAISS vector store\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "vectorstore.save_local(\"bop_vectorstore\")\n",
        "\n",
        "print(f\"Vector store created with {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-r463d5hYXO",
        "outputId": "16c8dd08-244f-4c2b-974a-7171f977de18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-39eb65970e71>:15: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created with 168 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Load vectorstore\n",
        "db = FAISS.load_local(\"bop_vectorstore\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Example: retrieve guidelines for coffee shops\n",
        "query = \"What are the underwriting guidelines for coffee shops?\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "# Pretty print retrieved chunks\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Document {i+1} ---\")\n",
        "    print(doc.page_content[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ofazptAhgXE",
        "outputId": "16401b58-4c7d-4fe6-d5eb-a6a799654dfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-bc9e917a3c84>:12: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Document 1 ---\n",
            "3.40\n",
            "Coffee Shops & Tea Houses\n",
            "Underwriting Appetite\n",
            "• Preferred:\n",
            "– Area ≤7,500 sq ft, seating ≤150\n",
            "– No alcohol service or live entertainment\n",
            "– Operates year-round\n",
            "– Strong fire safety and loss prevention\n",
            "• Acceptable:\n",
            "– Incidental music\n",
            "– Limited alcohol sales (<25%)\n",
            "– Catering up to 15% of total sales\n",
            "• Not Acceptable:\n",
            "– Alcohol-focused operations (bars)\n",
            "– Live entertainment or seasonal closures > 30 days\n",
            "– Large venues or poor fire controls\n",
            "Key Risk Considerations\n",
            "• Fire from coffee equipmen\n",
            "--- Document 2 ---\n",
            "Key Risk Considerations\n",
            "• Misrepresentation, errors in transactions, tenant disputes\n",
            "Industry-Specific Restrictions\n",
            "• Property management excluded\n",
            "• Coverage based on BPP and location\n",
            "Preferred Optional Coverages\n",
            "• Cyber, EPLI, D&O Liability\n",
            "• Commercial Auto and Business Interruption\n",
            "3.99\n",
            "Restaurants (Full Service, Limited Service, Fast Food)\n",
            "Underwriting Appetite\n",
            "• Preferred:\n",
            "– <7,500 sq ft, ≤150 seats\n",
            "– Controlled alcohol and catering percentages\n",
            "– NFPA-compliant suppression systems\n",
            "• Accepta\n",
            "--- Document 3 ---\n",
            "3.69\n",
            "Gift & Souvenir Shops\n",
            "Underwriting Appetite\n",
            "• Preferred:\n",
            "– Retail locations with >3 years in business and no claims\n",
            "– Stores in tourist areas with strong service culture\n",
            "• Acceptable:\n",
            "– Stores offering hobby/craft/gift items\n",
            "– Mixed online and storefront operations\n",
            "• Not Acceptable:\n",
            "– Stores selling hazardous or high-risk items\n",
            "– Less than 1 year in operation without controls\n",
            "Key Risk Considerations\n",
            "• Theft and inventory spoilage\n",
            "• Seasonal sales fluctuation\n",
            "• Slip-and-fall liability\n",
            "Indust\n",
            "--- Document 4 ---\n",
            "Preferred Optional Coverages\n",
            "• Equipment Breakdown Insurance\n",
            "• Employee Dishonesty Coverage\n",
            "• Cyber Liability Insurance\n",
            "• Glass and Business Interruption Insurance\n",
            "3.83\n",
            "Libraries & Museums\n",
            "Underwriting Appetite\n",
            "• Preferred:\n",
            "– Small community spaces with strong risk controls\n",
            "– No alcohol service or frequent large events\n",
            "• Acceptable:\n",
            "– Medium-size operations with occasional events\n",
            "• Not Acceptable:\n",
            "– High-traffic venues with security deficiencies\n",
            "– Frequent alcohol service or bar operation\n",
            "Key Ri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CZVeyjiphgLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the CSV file\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"BOP Policy Submission Details - BOP Submission Elements.csv\")\n",
        "\n",
        "# Step 2: Format the required fields into a description list\n",
        "field_descriptions = \"\\n\".join(\n",
        "    f\"- {row['Element Name']}: {row['Description']}\"\n",
        "    for _, row in df.iterrows()\n",
        ")"
      ],
      "metadata": {
        "id": "fYYzEuqEkOGX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Load Vector Store\n",
        "vectorstore = FAISS.load_local(\"bop_vectorstore\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# LLM Setup\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
        "\n",
        "# Step 1: Retrieve Guidelines\n",
        "def get_guidelines(business_type: str) -> str:\n",
        "    query = f\"Underwriting guidelines for {business_type}\"\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Step 2: Generate Questions\n",
        "question_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"guidelines\"],\n",
        "    template=\"\"\"\n",
        "You are a commercial insurance underwriter.\n",
        "\n",
        "Given the business type \"{business_type}\" and the following underwriting guidelines:\n",
        "\n",
        "{guidelines}\n",
        "\n",
        "Generate a list of 5 to 10 key underwriting questions that would help assess eligibility or risk for this business. Use a mix of yes/no and short-answer formats. Each question should reflect a specific requirement or risk consideration.\n",
        "\"\"\"\n",
        ")\n",
        "question_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
        "\n",
        "# Step 3: Generate Answers\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are simulating answers to an insurance application for a \"{business_type}\".\n",
        "\n",
        "Here are the underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Generate realistic, internally consistent answers for each question. Use natural business behavior and practical values. Provide short but detailed answers.\n",
        "\"\"\"\n",
        ")\n",
        "answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
        "\n",
        "# Step 4: Define an application generation prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "application_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"qa_pairs\", \"fields_to_fill\"],\n",
        "    template=\"\"\"\n",
        "You are filling out a Business Owner Policy (BOP) insurance application for a \"{business_type}\".\n",
        "\n",
        "Based on the following answered underwriting questions, generate a complete and realistic application record. Even if a field is not mentioned in the questions, infer it using common industry knowledge and consistency. Do not leave any values blank.\n",
        "\n",
        "You must fill in the following fields:\n",
        "{fields_to_fill}\n",
        "\n",
        "Answered Questions:\n",
        "{qa_pairs}\n",
        "\n",
        "Return the application as a well-structured JSON object.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
        "application_chain = LLMChain(llm=llm, prompt=application_prompt)\n",
        "\n",
        "# Step 5: run function\n",
        "def generate_bop_application(business_type: str, qa_pairs: str) -> dict:\n",
        "    response = application_chain.invoke({\n",
        "        \"business_type\": business_type,\n",
        "        \"qa_pairs\": qa_pairs,\n",
        "        \"fields_to_fill\": field_descriptions\n",
        "    })\n",
        "    return response[\"text\"]  # or return json.loads(response[\"text\"]) if it's valid JSON\n",
        "\n",
        "# Run the pipeline\n",
        "the_biz = \"Manufacturing technologies\"\n",
        "guidelines = get_guidelines(the_biz)\n",
        "\n",
        "questions = question_chain.invoke({\n",
        "    \"business_type\": the_biz,\n",
        "    \"guidelines\": guidelines\n",
        "})[\"text\"]\n",
        "\n",
        "answers = answer_chain.invoke({\n",
        "    \"business_type\": the_biz,\n",
        "    \"questions\": questions\n",
        "})[\"text\"]\n",
        "\n",
        "qa_pairs = \"\\n\".join(\n",
        "    f\"{q.strip()} — {a.strip()}\"\n",
        "    for q, a in zip(questions.strip().split(\"\\n\"), answers.strip().split(\"\\n\"))\n",
        "    if q.strip() and a.strip()\n",
        ")\n",
        "\n",
        "application_json = generate_bop_application(the_biz, qa_pairs)\n",
        "\n",
        "# === Print the result ===\n",
        "import json\n",
        "print(\"Underwriting Questions:\\n\", questions, \"\\n\")\n",
        "print(\"Simulated Answers:\\n\", answers, \"\\n\")\n",
        "print(\"Final Application JSON:\\n\", json.dumps(json.loads(application_json), indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuxCfRInibBi",
        "outputId": "2529f78a-32dd-4694-f461-1f289bf8141f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a6daa4ab0a54>:12: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
            "<ipython-input-10-a6daa4ab0a54>:33: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  question_chain = LLMChain(llm=llm, prompt=question_prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Underwriting Questions:\n",
            " 1. How many locations does your manufacturing technologies business operate from? \n",
            "2. Has the building's core systems or roofing been updated in the recent past? If yes, when was it last updated?\n",
            "3. Does the business involve any emergency response, LPG work, or manufacturing activities? If yes, please elaborate.\n",
            "4. Does the business maintain a sensitive customer database? If yes, is there an existing cyber liability insurance coverage?\n",
            "5. What is the estimated value of your inventory? \n",
            "6. Does the business have any online sales? If yes, what percentage of your revenue is derived from online sales?\n",
            "7. How many claims has the business filed in the past three years?\n",
            "8. Does your business operate in a disaster-prone area? If yes, do you have flood/wind/earthquake coverage?\n",
            "9. Do you have a safety program in place for your employees? If yes, please provide brief details about it.\n",
            "10. Does your business involve any high-pressure boiler work or hazardous site exposure? If yes, please elaborate. \n",
            "11. Does your business offer consulting or design services? If yes, do you have professional liability insurance coverage? \n",
            "12. Are there any off-premises services that account for more than 25% of your revenue? \n",
            "13. Does the business use subcontractors? If yes, could you provide the documentation showing their indemnification and insurance requirements?\n",
            "14. Does the business have an alarm system installed for high-value inventory locations? \n",
            "\n",
            "Simulated Answers:\n",
            " 1. Our manufacturing technologies business operates from three locations.\n",
            "2. Yes, the building's core systems and roofing have been updated. The last update occurred in 2018.\n",
            "3. Yes, our business involves manufacturing activities. We manufacture and assemble high-tech machine components.\n",
            "4. Yes, the business maintains a sensitive customer database. We do have existing cyber liability insurance coverage to protect against data breaches.\n",
            "5. The estimated value of our inventory is approximately $2 million.\n",
            "6. Yes, the business has online sales, which contribute to about 30% of our total revenue.\n",
            "7. The business has filed two claims in the past three years, both related to minor equipment malfunctions.\n",
            "8. No, our business does not operate in a disaster-prone area.\n",
            "9. Yes, we have a comprehensive safety program in place. It includes regular safety training, provision of safety equipment, and routine safety checks.\n",
            "10. No, our business does not involve any high-pressure boiler work or hazardous site exposure.\n",
            "11. Yes, we offer design services and we have professional liability insurance coverage to protect against potential negligence claims.\n",
            "12. No, there are no off-premises services that account for more than 25% of our revenue.\n",
            "13. Yes, we use subcontractors. We can provide documentation showing their indemnification and insurance requirements upon request.\n",
            "14. Yes, the business has a sophisticated alarm system installed in all locations where high-value inventory is stored. \n",
            "\n",
            "Final Application JSON:\n",
            " {\n",
            "  \"NAME\": \"Manufacturing Technologies Inc.\",\n",
            "  \"FEIN OR SOC SEC #\": \"12-3456789\",\n",
            "  \"BUSINESS TYPE\": \"Corporation\",\n",
            "  \"MAILING ADDRESS\": \"123 Manufacturing Lane, Techville, State, 12345-6789\",\n",
            "  \"CONTACT FOR INSPECTION\": \"John Doe, Operations Manager\",\n",
            "  \"GL CODE\": \"59911\",\n",
            "  \"SIC\": \"3541\",\n",
            "  \"NATURE OF BUSINESS\": \"Manufacture and assembly of high-tech machine components\",\n",
            "  \"DESCRIPTION OF OPERATIONS\": \"Manufacturing, assembling, and shipping high-tech machine components. Online sales account for 30% of revenue. Design services offered.\",\n",
            "  \"DATE BUSINESS STARTED\": \"01/01/2000\",\n",
            "  \"EFFECTIVE DATE\": \"01/01/2022\",\n",
            "  \"EXPIRATION DATE\": \"12/31/2021\",\n",
            "  \"NEW/RENEWAL\": \"Renewal\",\n",
            "  \"PAYMENT PLAN\": \"Annual\",\n",
            "  \"TOTAL PREMIUM\": \"$22,000\",\n",
            "  \"POLICY #\": \"BOP123456\",\n",
            "  \"GENERAL INFO QUESTIONS\": {\n",
            "    \"Hazardous materials\": \"No\",\n",
            "    \"Athletic teams sponsored\": \"No\",\n",
            "    \"Subcontractors insurance\": \"Yes, Operations Manager checks certificates\",\n",
            "    \"Convictions of fraud, bribery or arson\": \"No\",\n",
            "    \"Coverage declined, canceled, or non-renewed\": \"No\",\n",
            "    \"Leasing employees\": \"No\",\n",
            "    \"Workers compensation\": \"Yes\",\n",
            "    \"Other businesses\": \"No\",\n",
            "    \"Other insurance with this company\": \"No\",\n",
            "    \"Manufacturing, mixing, relabeling or repackaging\": \"Yes, manufacturing\",\n",
            "    \"Renting or loaning equipment\": \"No\",\n",
            "    \"Foreclosure, repossession, bankruptcy, judgment, or lien\": \"No\",\n",
            "    \"Exposure to flammables, explosives, or chemicals\": \"No\",\n",
            "    \"Catastrophe exposure\": \"No\",\n",
            "    \"Sexual abuse or molestation allegations, discrimination, or negligent hiring\": \"No\",\n",
            "    \"Uncorrected fire code violations\": \"No\"\n",
            "  },\n",
            "  \"OTHER BUSINESSES OWNED\": \"None\",\n",
            "  \"PREVIOUS CARRIER\": \"Insurance Co. Ltd.\",\n",
            "  \"POLICY NUMBER\": \"BOP654321\",\n",
            "  \"TOTAL LOSSES\": \"$5,000\",\n",
            "  \"DESCRIPTION OF LOSSES\": \"Two minor equipment malfunctions. One in 2019 for $2,500 and another in 2020 for $2,500. Both claims settled.\",\n",
            "  \"ADDRESS\": \"123 Manufacturing Lane, Techville, State, 12345\",\n",
            "  \"Year Built\": \"1995\",\n",
            "  \"Square Feet\": \"10,000\",\n",
            "  \"HOURS OF OPERATION\": \"Mon-Fri, 9am-5pm\",\n",
            "  \"Distance to hydrant\": \"500\",\n",
            "  \"OCCUPANCY\": \"Manufacturing\",\n",
            "  \"BUILDING IMPROVEMENTS\": \"2018\",\n",
            "  \"BASEMENT PRESENT\": \"No\",\n",
            "  \"SPRINKLER PERCENTAGE\": \"100%\",\n",
            "  \"ANNUAL SALES\": \"$6,000,000\",\n",
            "  \"TOTAL PAYROLL\": \"$2,000,000\",\n",
            "  \"NO. OF EMPLOYEES\": \"50\",\n",
            "  \"ALARM TYPE\": \"Police connected\",\n",
            "  \"SAFE/VAULT DETAILS\": \"Grade A safe\",\n",
            "  \"MONEY ON PREMISES\": \"$1,000\",\n",
            "  \"OTHER PROTECTION\": \"Fencing, CCTV, security guard\",\n",
            "  \"Building limit\": \"$1,500,000\",\n",
            "  \"building % Coinsurance\": \"80%\",\n",
            "  \"personal property valuation type\": \"Replacement cost\",\n",
            "  \"personal property limit\": \"$2,000,000\",\n",
            "  \"personal property % Coinsurance\": \"80%\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "# === Load Data ===\n",
        "coverages_df = pd.read_csv(\"BOP Coverages - coverages.csv\")\n",
        "sic_codes_df = pd.read_csv(\"sic-codes.csv\")\n",
        "fields_df = pd.read_csv(\"BOP Policy Submission Details - BOP Submission Elements.csv\")\n",
        "\n",
        "# === Prepare Submission Fields ===\n",
        "all_submission_fields = fields_df[\"Element Name\"].drop_duplicates().tolist()\n",
        "fields_to_fill = \"\\n\".join(all_submission_fields)\n",
        "\n",
        "# === LLM and Vector Store Setup ===\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
        "vectorstore = FAISS.load_local(\"bop_vectorstore\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# === Prompt Templates ===\n",
        "question_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"guidelines\"],\n",
        "    template=\"\"\"\n",
        "You are a commercial insurance underwriter.\n",
        "\n",
        "Given the business type \"{business_type}\" and the following underwriting guidelines:\n",
        "\n",
        "{guidelines}\n",
        "\n",
        "Generate 5 to 10 underwriting questions that would help assess eligibility or risk. Use yes/no and short-answer formats.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are simulating answers to an insurance application for a \"{business_type}\".\n",
        "\n",
        "Here are the underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Provide realistic, internally consistent answers. Use natural behavior and practical values.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "owner_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are a business owner who owns a \"{business_type}\" type of business.\n",
        "\n",
        "Here are underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Now answer:\n",
        "- What does your business do?\n",
        "- Who are your customers?\n",
        "- How many employees do you have (full/part-time)?\n",
        "- What is your annual payroll?\n",
        "- Where do you operate (city/neighborhood)?\n",
        "- Do you have a physical location (size, features)?\n",
        "- Anything unique or special?\n",
        "- How do you make money?\n",
        "- What is your annual gross sales?\n",
        "\n",
        "Be natural, realistic, and detailed.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "application_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"qa_pairs\", \"fields_to_fill\", \"business_owner_description\"],\n",
        "    template=\"\"\"\n",
        "You are completing a Business Owner Policy (BOP) insurance application for a \"{business_type}\".\n",
        "\n",
        "Use these answered questions and business owner description to complete it:\n",
        "\n",
        "Answered Questions:\n",
        "{qa_pairs}\n",
        "\n",
        "Business Description:\n",
        "{business_owner_description}\n",
        "\n",
        "Do not leave any values blank.\n",
        "\n",
        "You must fill in the following fields:\n",
        "{fields_to_fill}\n",
        "\n",
        "Return the application as a well-structured JSON object.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# === Chains ===\n",
        "question_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
        "answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
        "owner_chain = LLMChain(llm=llm, prompt=owner_prompt)\n",
        "application_chain = LLMChain(llm=llm, prompt=application_prompt)\n",
        "\n",
        "# === Utility Functions ===\n",
        "def get_guidelines(business_type: str) -> str:\n",
        "    docs = retriever.get_relevant_documents(f\"Underwriting guidelines for {business_type}\")\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs[:3])\n",
        "\n",
        "# === Main Function ===\n",
        "def generate_bop_application(business_type: str) -> dict:\n",
        "    with get_openai_callback() as cb:\n",
        "        guidelines = get_guidelines(business_type)\n",
        "\n",
        "        questions = question_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"guidelines\": guidelines\n",
        "        })[\"text\"]\n",
        "\n",
        "        answers = answer_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        qa_pairs = \"\\n\".join(\n",
        "            f\"{q.strip()} — {a.strip()}\"\n",
        "            for q, a in zip(questions.strip().split(\"\\n\"), answers.strip().split(\"\\n\"))\n",
        "            if q.strip() and a.strip()\n",
        "        )\n",
        "\n",
        "        owner_description = owner_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        final_app = application_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"qa_pairs\": qa_pairs,\n",
        "            \"fields_to_fill\": fields_to_fill,\n",
        "            \"business_owner_description\": owner_description\n",
        "        })[\"text\"]\n",
        "\n",
        "        print(f\"Total tokens used: {cb.total_tokens}\")\n",
        "        return json.loads(final_app)\n",
        "\n",
        "# === Example Run ===\n",
        "if __name__ == \"__main__\":\n",
        "    biz_type = \"Manufacturing Technologies\"\n",
        "    app = generate_bop_application(biz_type)\n",
        "    print(json.dumps(app, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXbHZcWef0Ec",
        "outputId": "3c56cfec-64a3-46d2-c027-7b539ae65b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens used: 4082\n",
            "{\n",
            "  \"name\": \"Manufacturing Technologies\",\n",
            "  \"fein_or_soc_sec\": \"123-45-6789\",\n",
            "  \"business_type\": \"Industrial equipment and machinery manufacturing\",\n",
            "  \"mailing_address\": \"123 Industrial Park Drive, Lexington, KY 40511\",\n",
            "  \"contact_for_inspection\": \"John Doe, CEO\",\n",
            "  \"gl_code\": \"61224\",\n",
            "  \"sic\": \"3569\",\n",
            "  \"nature_of_business\": \"Manufacturing\",\n",
            "  \"description_of_operations\": \"Design, development, and manufacturing of specialized machinery and equipment used in various industrial sectors.\",\n",
            "  \"date_business_started\": \"01/01/2000\",\n",
            "  \"effective_date\": \"01/01/2022\",\n",
            "  \"expiration_date\": \"01/01/2023\",\n",
            "  \"new_renewal\": \"Renewal\",\n",
            "  \"payment_plan\": \"Annual\",\n",
            "  \"total_premium\": \"$10,000\",\n",
            "  \"policy_number\": \"XYZ789101\",\n",
            "  \"general_info_questions\": {\n",
            "    \"more_than_four_locations\": \"No\",\n",
            "    \"outdated_systems_or_roofing\": \"No\",\n",
            "    \"high_risk_manufacturing\": \"No\",\n",
            "    \"current_insurance_coverage\": \"General liability insurance, property insurance, workers' compensation insurance\",\n",
            "    \"sensitive_customer_database\": \"Yes\",\n",
            "    \"inventory_value_or_online_sales\": \"Yes\",\n",
            "    \"claims_in_past_three_years\": \"No\",\n",
            "    \"disaster_prone_area\": \"No\",\n",
            "    \"disaster_coverage\": \"Not Applicable\",\n",
            "    \"high_pressure_or_hazardous_exposure\": \"No\"\n",
            "  },\n",
            "  \"other_businesses_owned\": \"None\",\n",
            "  \"previous_carrier\": \"XYZ Insurance Co.\",\n",
            "  \"total_losses\": \"1\",\n",
            "  \"description_of_losses\": \"Workplace accident resulting in minor injury\",\n",
            "  \"combined_single_limit\": \"$1,000,000\",\n",
            "  \"bodily_injury_prop_damage_occurrence\": \"$500,000\",\n",
            "  \"bodily_injury_prop_damage_aggregate\": \"$2,000,000\",\n",
            "  \"medical_expense\": \"$5,000\",\n",
            "  \"damage_to_rental_premises\": \"$100,000\",\n",
            "  \"professional_liability\": \"$1,000,000\",\n",
            "  \"liquor_liability\": \"Not Applicable\",\n",
            "  \"hired_auto\": \"$500,000\",\n",
            "  \"non_owned_auto\": \"$500,000\",\n",
            "  \"computers\": \"$200,000\",\n",
            "  \"ordinance_and_law\": \"$100,000\",\n",
            "  \"flood\": \"Not Applicable\",\n",
            "  \"accounts_receivable\": \"$500,000\",\n",
            "  \"employee_dishonesty\": \"$100,000\",\n",
            "  \"money_and_securities\": \"$50,000\",\n",
            "  \"address\": \"123 Industrial Park Drive, Lexington, KY 40511\",\n",
            "  \"year_built\": \"2010\",\n",
            "  \"square_feet\": \"50,000\",\n",
            "  \"hours_of_operation\": \"8:00 AM - 5:00 PM\",\n",
            "  \"distance_to_hydrant\": \"< 1000 feet\",\n",
            "  \"occupancy\": \"Fully Occupied\",\n",
            "  \"building_improvements\": \"Updated core systems, new roofing\",\n",
            "  \"basement_present\": \"No\",\n",
            "  \"sprinkler_percentage\": \"100%\",\n",
            "  \"annual_sales\": \"$15,000,000\",\n",
            "  \"total_payroll\": \"$4,800,000\",\n",
            "  \"no_of_employees\": \"85\",\n",
            "  \"alarm_type\": \"Central Station\",\n",
            "  \"safe_vault_details\": \"Fireproof safe on premises\",\n",
            "  \"money_on_premises\": \"< $5,000\",\n",
            "  \"other_protection\": \"Surveillance cameras, security personnel\",\n",
            "  \"building_limit\": \"$5,000,000\",\n",
            "  \"building_coinsurance\": \"80%\",\n",
            "  \"personal_property_valuation_type\": \"Replacement cost\",\n",
            "  \"personal_property_limit\": \"$1,000,000\",\n",
            "  \"personal_property_coinsurance\": \"80%\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "# === Load Data ===\n",
        "coverages_df = pd.read_csv(\"BOP Coverages - coverages.csv\")\n",
        "sic_codes_df = pd.read_csv(\"sic-codes.csv\")\n",
        "fields_df = pd.read_csv(\"BOP Policy Submission Details - BOP Submission Elements.csv\")\n",
        "\n",
        "# === Prepare Submission Fields ===\n",
        "all_submission_fields = fields_df[\"Element Name\"].drop_duplicates().tolist()\n",
        "fields_to_fill = \"\\n\".join(all_submission_fields)\n",
        "\n",
        "# === Business Categories ===\n",
        "with open(\"bop_categories.txt\") as f:\n",
        "    bop_categories = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "bop_categories = bop_categories[:10]\n",
        "\n",
        "# === LLM and Vector Store Setup ===\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
        "vectorstore = FAISS.load_local(\"bop_vectorstore\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# === Prompt Templates ===\n",
        "question_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"guidelines\"],\n",
        "    template=\"\"\"\n",
        "You are a commercial insurance underwriter.\n",
        "\n",
        "Given the business type \"{business_type}\" and the following underwriting guidelines:\n",
        "\n",
        "{guidelines}\n",
        "\n",
        "Generate 5 to 10 underwriting questions that would help assess eligibility or risk. Use yes/no and short-answer formats.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are simulating answers to an insurance application for a \"{business_type}\".\n",
        "\n",
        "Here are the underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Provide realistic, internally consistent answers. Use natural behavior and practical values.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "owner_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are a business owner who owns a \"{business_type}\" type of business.\n",
        "\n",
        "Here are underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Now answer:\n",
        "- What does your business do?\n",
        "- Who are your customers?\n",
        "- How many employees do you have (full/part-time)?\n",
        "- What is your annual payroll?\n",
        "- Where do you operate (city/neighborhood)?\n",
        "- Do you have a physical location (size, features)?\n",
        "- Anything unique or special?\n",
        "- How do you make money?\n",
        "- What is your annual gross sales?\n",
        "\n",
        "Be natural, realistic, and detailed.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "application_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"qa_pairs\", \"fields_to_fill\", \"business_owner_description\"],\n",
        "    template=\"\"\"\n",
        "You are completing a Business Owner Policy (BOP) insurance application for a \"{business_type}\".\n",
        "\n",
        "Use these answered questions and business owner description to complete it:\n",
        "\n",
        "Answered Questions:\n",
        "{qa_pairs}\n",
        "\n",
        "Business Description:\n",
        "{business_owner_description}\n",
        "\n",
        "Do not leave any values blank.\n",
        "\n",
        "You must fill in the following fields:\n",
        "{fields_to_fill}\n",
        "\n",
        "Return the application as a well-structured JSON object.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# === Chains ===\n",
        "question_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
        "answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
        "owner_chain = LLMChain(llm=llm, prompt=owner_prompt)\n",
        "application_chain = LLMChain(llm=llm, prompt=application_prompt)\n",
        "\n",
        "# === Utility Functions ===\n",
        "def get_guidelines(business_type: str) -> str:\n",
        "    docs = retriever.get_relevant_documents(f\"Underwriting guidelines for {business_type}\")\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs[:3])\n",
        "\n",
        "def simulate_credit_score() -> dict:\n",
        "    r = random.random()\n",
        "    if r < 0.2:\n",
        "        return {\"score\": random.randint(450, 599), \"rating\": \"Low\"}\n",
        "    elif r < 0.6:\n",
        "        return {\"score\": random.randint(600, 699), \"rating\": \"Average\"}\n",
        "    else:\n",
        "        return {\"score\": random.randint(700, 850), \"rating\": \"Good\"}\n",
        "\n",
        "def simulate_google_reviews(business_name: str) -> dict:\n",
        "    review_count = random.randint(5, 250)\n",
        "    rating = round(random.uniform(1.0, 5.0), 1)\n",
        "    all_reviews = [\n",
        "        \"Great customer service and reliable work!\",\n",
        "        \"Mediocre experience, not bad but could be better.\",\n",
        "        \"I had to wait too long, not coming back.\",\n",
        "        \"Wonderful staff and professional results.\",\n",
        "        \"Overpriced and disappointing experience.\"\n",
        "    ]\n",
        "    if random.random() < 0.1:\n",
        "        samples = random.choices(all_reviews[-2:], k=5)\n",
        "    else:\n",
        "        samples = random.choices(all_reviews, k=5)\n",
        "    return {\n",
        "        \"review_count\": review_count,\n",
        "        \"average_rating\": rating,\n",
        "        \"reviews\": samples\n",
        "    }\n",
        "\n",
        "def simulate_claims_history(credit_score: int, avg_rating: float) -> dict:\n",
        "    if credit_score < 600 or avg_rating < 2.5:\n",
        "        num_claims = random.randint(2, 5)\n",
        "    elif credit_score < 700 or avg_rating < 3.5:\n",
        "        num_claims = random.randint(1, 3)\n",
        "    else:\n",
        "        num_claims = random.choice([0, 1])\n",
        "    total_losses = num_claims * random.randint(1000, 10000)\n",
        "    return {\n",
        "        \"num_claims\": num_claims,\n",
        "        \"total_losses\": total_losses\n",
        "    }\n",
        "\n",
        "# === Batch Simulation ===\n",
        "def batch_generate_applications(n_per_category=1):\n",
        "    applications = []\n",
        "    external_data = []\n",
        "    for category in bop_categories:\n",
        "        for i in range(n_per_category):\n",
        "            try:\n",
        "                business_type = f\"{category} - {uuid.uuid4().hex[:6]}\"\n",
        "                app = generate_bop_application(business_type)\n",
        "                credit = simulate_credit_score()\n",
        "                reviews = simulate_google_reviews(app.get(\"NAME\", business_type))\n",
        "                claims = simulate_claims_history(credit[\"score\"], reviews[\"average_rating\"])\n",
        "\n",
        "                applications.append(app)\n",
        "                external_data.append({\n",
        "                    \"Business Name\": app.get(\"NAME\", business_type),\n",
        "                    \"Credit Score\": credit[\"score\"],\n",
        "                    \"Credit Rating\": credit[\"rating\"],\n",
        "                    \"Google Review Count\": reviews[\"review_count\"],\n",
        "                    \"Average Review Rating\": reviews[\"average_rating\"],\n",
        "                    \"Sample Reviews\": reviews[\"reviews\"],\n",
        "                    \"Number of Claims\": claims[\"num_claims\"],\n",
        "                    \"Total Losses\": claims[\"total_losses\"]\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating for {category}: {e}\")\n",
        "\n",
        "    pd.DataFrame(applications).to_json(\"/mnt/data/generated_bop_applications.json\", orient=\"records\", indent=2)\n",
        "    pd.DataFrame(external_data).to_json(\"/mnt/data/generated_bop_third_party_data.json\", orient=\"records\", indent=2)\n",
        "    print(\"Batch generation completed.\")\n",
        "\n",
        "# === Main Function ===\n",
        "def generate_bop_application(business_type: str) -> dict:\n",
        "    with get_openai_callback() as cb:\n",
        "        guidelines = get_guidelines(business_type)\n",
        "\n",
        "        questions = question_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"guidelines\": guidelines\n",
        "        })[\"text\"]\n",
        "\n",
        "        answers = answer_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        qa_pairs = \"\\n\".join(\n",
        "            f\"{q.strip()} — {a.strip()}\"\n",
        "            for q, a in zip(questions.strip().split(\"\\n\"), answers.strip().split(\"\\n\"))\n",
        "            if q.strip() and a.strip()\n",
        "        )\n",
        "\n",
        "        owner_description = owner_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        final_app = application_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"qa_pairs\": qa_pairs,\n",
        "            \"fields_to_fill\": fields_to_fill,\n",
        "            \"business_owner_description\": owner_description\n",
        "        })[\"text\"]\n",
        "\n",
        "        print(f\"Total tokens used: {cb.total_tokens}\")\n",
        "        return json.loads(final_app)\n",
        "\n",
        "# === Example Run ===\n",
        "if __name__ == \"__main__\":\n",
        "    batch_generate_applications(n_per_category=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "ipSLl3ykhpr4",
        "outputId": "414ffbf4-25a3-45de-a0ed-abc14281afdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-c78f99f990c0>:30: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
            "<ipython-input-7-c78f99f990c0>:106: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  question_chain = LLMChain(llm=llm, prompt=question_prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens used: 4340\n",
            "Total tokens used: 3965\n",
            "Total tokens used: 4180\n",
            "Total tokens used: 4534\n",
            "Total tokens used: 3845\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c78f99f990c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;31m# === Example Run ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mbatch_generate_applications\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_per_category\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-c78f99f990c0>\u001b[0m in \u001b[0;36mbatch_generate_applications\u001b[0;34m(n_per_category)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mbusiness_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{category} - {uuid.uuid4().hex[:6]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_bop_application\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusiness_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mcredit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_credit_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_google_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NAME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusiness_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c78f99f990c0>\u001b[0m in \u001b[0;36mgenerate_bop_application\u001b[0;34m(business_type)\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         owner_description = owner_chain.invoke({\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;34m\"business_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbusiness_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;34m\"questions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             outputs = (\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     ) -> dict[str, str]:\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    946\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                 results.append(\n\u001b[0;32m--> 766\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    767\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1013\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         }\n\u001b[0;32m--> 476\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    970\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "# === Load Data ===\n",
        "coverages_df = pd.read_csv(\"BOP Coverages - coverages.csv\")\n",
        "sic_codes_df = pd.read_csv(\"sic-codes.csv\")\n",
        "fields_df = pd.read_csv(\"BOP Policy Submission Details - BOP Submission Elements.csv\")\n",
        "\n",
        "# === Prepare Submission Fields ===\n",
        "all_submission_fields = fields_df[\"Element Name\"].drop_duplicates().tolist()\n",
        "fields_to_fill = \"\\n\".join(all_submission_fields)\n",
        "\n",
        "# === Business Categories ===\n",
        "with open(\"bop_categories.txt\") as f:\n",
        "    bop_categories = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "\n",
        "bop_categories = bop_categories[:4]\n",
        "# === LLM and Vector Store Setup ===\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
        "vectorstore = FAISS.load_local(\"bop_vectorstore\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "llm2 = ChatOpenAI(temperature=0.9, model_name=\"gpt-4\")\n",
        "\n",
        "# === Prompt Templates ===\n",
        "question_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"guidelines\"],\n",
        "    template=\"\"\"\n",
        "You are a commercial insurance underwriter.\n",
        "\n",
        "Given the business type \"{business_type}\" and the following underwriting guidelines:\n",
        "\n",
        "{guidelines}\n",
        "\n",
        "Generate 5 to 10 underwriting questions that would help assess eligibility or risk. Use yes/no and short-answer formats.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are simulating answers to an insurance application for a \"{business_type}\".\n",
        "\n",
        "Here are the underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Provide realistic, internally consistent answers. Use natural behavior and practical values.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "owner_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are a business owner who owns a \"{business_type}\" type of business.\n",
        "\n",
        "Here are underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Now answer:\n",
        "- What does your business do?\n",
        "- Who are your customers?\n",
        "- How many employees do you have (full/part-time)?\n",
        "- What is your annual payroll?\n",
        "- Where do you operate (city/neighborhood)?\n",
        "- Do you have a physical location (size, features)?\n",
        "- Anything unique or special?\n",
        "- How do you make money?\n",
        "- What is your annual gross sales?\n",
        "\n",
        "Be natural, realistic, and detailed.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "review_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"tone\"],\n",
        "    template=\"\"\"\n",
        "You are writing a short customer review for a business of type \"{business_type}\".\n",
        "Tone: {tone}\n",
        "Write a realistic one-sentence review specific to this type of business.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "application_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"qa_pairs\", \"fields_to_fill\", \"business_owner_description\"],\n",
        "    template=\"\"\"\n",
        "You are filling out a Business Owner Policy (BOP) insurance application for a \\\"{business_type}\\\".\n",
        "\n",
        "Based on the following answered underwriting questions and business owner description, generate a complete and realistic application record.\n",
        "Even if a field is not mentioned in the input, infer it using common industry knowledge and logical consistency.\n",
        "Do not leave any values blank. If a value is unknown, infer something plausible.\n",
        "\n",
        "You must fill in the following fields:\n",
        "{fields_to_fill}\n",
        "\n",
        "Answered Questions:\n",
        "{qa_pairs}\n",
        "\n",
        "Business Description:\n",
        "{business_owner_description}\n",
        "\n",
        "Return the application as a well-structured JSON object.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# === Chains ===\n",
        "question_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
        "answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
        "owner_chain = LLMChain(llm=llm, prompt=owner_prompt)\n",
        "application_chain = LLMChain(llm=llm, prompt=application_prompt)\n",
        "review_chain = LLMChain(llm=llm2, prompt=review_prompt)\n",
        "\n",
        "# === Utility Functions ===\n",
        "def get_guidelines(business_type: str) -> str:\n",
        "    docs = retriever.get_relevant_documents(f\"Underwriting guidelines for {business_type}\")\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs[:3])\n",
        "\n",
        "def simulate_credit_score() -> dict:\n",
        "    r = random.random()\n",
        "    if r < 0.2:\n",
        "        return {\"score\": random.randint(450, 599), \"rating\": \"Low\"}\n",
        "    elif r < 0.6:\n",
        "        return {\"score\": random.randint(600, 699), \"rating\": \"Average\"}\n",
        "    else:\n",
        "        return {\"score\": random.randint(700, 850), \"rating\": \"Good\"}\n",
        "\n",
        "def simulate_google_reviews(business_name: str, business_type: str) -> dict:\n",
        "    review_count = random.randint(5, 250)\n",
        "    rating = round(random.uniform(1.0, 5.0), 1)\n",
        "    tone = \"negative\" if random.random() < 0.1 else random.choices([\"positive\", \"neutral\", \"negative\"], weights=[0.5, 0.3, 0.2], k=5)\n",
        "    if isinstance(tone, list):\n",
        "        samples = [review_chain.invoke({\"business_type\": business_type, \"tone\": t})[\"text\"].strip() for t in tone]\n",
        "    else:\n",
        "        samples = [review_chain.invoke({\"business_type\": business_type, \"tone\": \"negative\"})[\"text\"].strip() for _ in range(5)]\n",
        "    return {\n",
        "        \"review_count\": review_count,\n",
        "        \"average_rating\": rating,\n",
        "        \"reviews\": samples\n",
        "    }\n",
        "\n",
        "def simulate_claims_history(credit_score: int, avg_rating: float) -> dict:\n",
        "    if credit_score < 600 or avg_rating < 2.5:\n",
        "        num_claims = random.randint(2, 5)\n",
        "    elif credit_score < 700 or avg_rating < 3.5:\n",
        "        num_claims = random.randint(1, 3)\n",
        "    else:\n",
        "        num_claims = random.choice([0, 1])\n",
        "    total_losses = num_claims * random.randint(1000, 10000)\n",
        "    return {\n",
        "        \"num_claims\": num_claims,\n",
        "        \"total_losses\": total_losses\n",
        "    }\n",
        "\n",
        "# === Batch Simulation ===\n",
        "def batch_generate_applications(n_per_category=1):\n",
        "    applications = []\n",
        "    external_data = []\n",
        "    for category in bop_categories:\n",
        "        for i in range(n_per_category):\n",
        "            try:\n",
        "                business_type = f\"{category} - {uuid.uuid4().hex[:6]}\"\n",
        "                app = generate_bop_application(business_type)\n",
        "                credit = simulate_credit_score()\n",
        "                reviews = simulate_google_reviews(app.get(\"NAME\", business_type), business_type)\n",
        "                claims = simulate_claims_history(credit[\"score\"], reviews[\"average_rating\"])\n",
        "\n",
        "                applications.append(app)\n",
        "                external_data.append({\n",
        "                    \"Business Name\": app.get(\"NAME\", business_type),\n",
        "                    \"Credit Score\": credit[\"score\"],\n",
        "                    \"Credit Rating\": credit[\"rating\"],\n",
        "                    \"Google Review Count\": reviews[\"review_count\"],\n",
        "                    \"Average Review Rating\": reviews[\"average_rating\"],\n",
        "                    \"Sample Reviews\": reviews[\"reviews\"],\n",
        "                    \"Number of Claims\": claims[\"num_claims\"],\n",
        "                    \"Total Losses\": claims[\"total_losses\"]\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating for {category}: {e}\")\n",
        "\n",
        "    pd.DataFrame(applications).to_json(\"generated_bop_applications.json\", orient=\"records\", indent=2)\n",
        "    pd.DataFrame(external_data).to_json(\"generated_bop_third_party_data.json\", orient=\"records\", indent=2)\n",
        "    print(\"Batch generation completed.\")\n",
        "\n",
        "# === Main Function ===\n",
        "def generate_bop_application(business_type: str) -> dict:\n",
        "    with get_openai_callback() as cb:\n",
        "        guidelines = get_guidelines(business_type)\n",
        "\n",
        "        questions = question_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"guidelines\": guidelines\n",
        "        })[\"text\"]\n",
        "\n",
        "        answers = answer_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        qa_pairs = \"\\n\".join(\n",
        "            f\"{q.strip()} — {a.strip()}\"\n",
        "            for q, a in zip(questions.strip().split(\"\\n\"), answers.strip().split(\"\\n\"))\n",
        "            if q.strip() and a.strip()\n",
        "        )\n",
        "\n",
        "        owner_description = owner_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        final_app = application_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"qa_pairs\": qa_pairs,\n",
        "            \"fields_to_fill\": fields_to_fill,\n",
        "            \"business_owner_description\": owner_description\n",
        "        })[\"text\"]\n",
        "\n",
        "        print(f\"Total tokens used: {cb.total_tokens}\")\n",
        "        return json.loads(final_app)\n",
        "\n",
        "# === Example Run ===\n",
        "if __name__ == \"__main__\":\n",
        "    batch_generate_applications(n_per_category=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1-Wfb_sXx7f",
        "outputId": "7087861b-110e-4c10-df52-85ce089684ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens used: 4318\n",
            "Total tokens used: 4204\n",
            "Total tokens used: 4448\n",
            "Total tokens used: 4087\n",
            "Batch generation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "# === Load Data ===\n",
        "coverages_df = pd.read_csv(\"BOP Coverages - coverages.csv\")\n",
        "sic_codes_df = pd.read_csv(\"sic-codes.csv\")\n",
        "fields_df = pd.read_csv(\"BOP Policy Submission Details - BOP Submission Elements.csv\")\n",
        "\n",
        "# === Prepare Submission Fields ===\n",
        "all_submission_fields = fields_df[\"Element Name\"].drop_duplicates().tolist()\n",
        "fields_to_fill = \"\\n\".join(all_submission_fields)\n",
        "\n",
        "# === Business Categories ===\n",
        "with open(\"bop_categories.txt\") as f:\n",
        "    bop_categories = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "bop_categories = bop_categories[:4]\n",
        "\n",
        "# === LLM Setup ===\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
        "llm2 = ChatOpenAI(temperature=0.9, model_name=\"gpt-4\")\n",
        "vectorstore = FAISS.load_local(\"bop_vectorstore\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# === Prompt Templates ===\n",
        "question_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"guidelines\"],\n",
        "    template=\"\"\"\n",
        "You are a commercial insurance underwriter.\n",
        "\n",
        "Given the business type \"{business_type}\" and the following underwriting guidelines:\n",
        "\n",
        "{guidelines}\n",
        "\n",
        "Generate 5 to 10 underwriting questions that would help assess eligibility or risk. Use yes/no and short-answer formats.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are simulating answers to an insurance application for a \"{business_type}\".\n",
        "\n",
        "Here are the underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Provide realistic, internally consistent answers. Use natural behavior and practical values.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "owner_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"questions\"],\n",
        "    template=\"\"\"\n",
        "You are a business owner who owns a \"{business_type}\" type of business.\n",
        "\n",
        "Here are underwriting questions:\n",
        "{questions}\n",
        "\n",
        "Now answer:\n",
        "- What does your business do?\n",
        "- Who are your customers?\n",
        "- How many employees do you have (full/part-time)?\n",
        "- What is your annual payroll?\n",
        "- Where do you operate (city/neighborhood)?\n",
        "- Do you have a physical location (size, features)?\n",
        "- Anything unique or special?\n",
        "- How do you make money?\n",
        "- What is your annual gross sales?\n",
        "\n",
        "Be natural, realistic, and detailed.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "review_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"tone\"],\n",
        "    template=\"\"\"\n",
        "You are writing a short customer review for a business of type \"{business_type}\".\n",
        "Tone: {tone}\n",
        "Write a realistic one-sentence review specific to this type of business.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "application_prompt = PromptTemplate(\n",
        "    input_variables=[\"business_type\", \"qa_pairs\", \"fields_to_fill\", \"business_owner_description\"],\n",
        "    template=\"\"\"\n",
        "You are filling out a Business Owner Policy (BOP) insurance application for a \"{business_type}\".\n",
        "\n",
        "Based on the following answered underwriting questions and business owner description, generate a complete and realistic application record.\n",
        "Even if a field is not mentioned in the input, infer it using common industry knowledge and logical consistency.\n",
        "Do not leave any values blank. If a value is unknown, infer something plausible.\n",
        "\n",
        "You must fill in the following fields:\n",
        "{fields_to_fill}\n",
        "\n",
        "Answered Questions:\n",
        "{qa_pairs}\n",
        "\n",
        "Business Description:\n",
        "{business_owner_description}\n",
        "\n",
        "Return the application as a well-structured JSON object.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# === Chains ===\n",
        "question_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
        "answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
        "owner_chain = LLMChain(llm=llm, prompt=owner_prompt)\n",
        "application_chain = LLMChain(llm=llm, prompt=application_prompt)\n",
        "review_chain = LLMChain(llm=llm2, prompt=review_prompt)\n",
        "\n",
        "# === Utility Functions ===\n",
        "def create_empty_application_template(path=\"empty_bop_template.json\"):\n",
        "    template = {field: None for field in all_submission_fields}\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(template, f, indent=2)\n",
        "    print(f\"Empty application template saved to {path}\")\n",
        "\n",
        "def get_guidelines(business_type: str) -> str:\n",
        "    docs = retriever.get_relevant_documents(f\"Underwriting guidelines for {business_type}\")\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs[:3])\n",
        "\n",
        "def simulate_credit_score() -> dict:\n",
        "    r = random.random()\n",
        "    if r < 0.2:\n",
        "        return {\"score\": random.randint(450, 599), \"rating\": \"Low\"}\n",
        "    elif r < 0.6:\n",
        "        return {\"score\": random.randint(600, 699), \"rating\": \"Average\"}\n",
        "    else:\n",
        "        return {\"score\": random.randint(700, 850), \"rating\": \"Good\"}\n",
        "\n",
        "def simulate_google_reviews(business_name: str, business_type: str) -> dict:\n",
        "    review_count = random.randint(5, 250)\n",
        "    rating = round(random.uniform(1.0, 5.0), 1)\n",
        "    tone = \"negative\" if random.random() < 0.1 else random.choices([\"positive\", \"neutral\", \"negative\"], weights=[0.5, 0.3, 0.2], k=5)\n",
        "    if isinstance(tone, list):\n",
        "        samples = [review_chain.invoke({\"business_type\": business_type, \"tone\": t})[\"text\"].strip() for t in tone]\n",
        "    else:\n",
        "        samples = [review_chain.invoke({\"business_type\": business_type, \"tone\": \"negative\"})[\"text\"].strip() for _ in range(5)]\n",
        "    return {\n",
        "        \"review_count\": review_count,\n",
        "        \"average_rating\": rating,\n",
        "        \"reviews\": samples\n",
        "    }\n",
        "\n",
        "def simulate_claims_history(credit_score: int, avg_rating: float) -> dict:\n",
        "    if credit_score < 600 or avg_rating < 2.5:\n",
        "        num_claims = random.randint(2, 5)\n",
        "    elif credit_score < 700 or avg_rating < 3.5:\n",
        "        num_claims = random.randint(1, 3)\n",
        "    else:\n",
        "        num_claims = random.choice([0, 1])\n",
        "    total_losses = num_claims * random.randint(1000, 10000)\n",
        "    return {\n",
        "        \"num_claims\": num_claims,\n",
        "        \"total_losses\": total_losses\n",
        "    }\n",
        "\n",
        "def validate_application(app: dict) -> dict:\n",
        "    required_keys = set(all_submission_fields)\n",
        "    flat_app = app if \"Application\" not in app else app[\"Application\"]\n",
        "\n",
        "    # Normalize field names for consistency\n",
        "    normalized_app = {k.strip().upper(): v for k, v in flat_app.items()}\n",
        "\n",
        "    # Basic field fixing\n",
        "    try:\n",
        "        premium_str = normalized_app.get(\"TOTAL PREMIUM\", \"$0\").replace(\"$\", \"\").replace(\",\", \"\")\n",
        "        premium_val = float(premium_str)\n",
        "        if premium_val > 500000:\n",
        "            normalized_app[\"TOTAL PREMIUM\"] = \"$250,000\"\n",
        "    except:\n",
        "        normalized_app[\"TOTAL PREMIUM\"] = \"$250,000\"\n",
        "\n",
        "    # Identify missing or implausible fields\n",
        "    missing_fields = [key for key in all_submission_fields if normalized_app.get(key) in [None, \"\", \"N/A\", \"Not Applicable\"]]\n",
        "    if missing_fields:\n",
        "        repair_prompt = f\"\"\"\n",
        "You are an expert in insurance underwriting. Here is a partially completed BOP application with missing or inconsistent fields:\n",
        "\n",
        "{json.dumps(normalized_app, indent=2)}\n",
        "\n",
        "Please provide plausible and realistic values to fill in the following fields:\n",
        "{missing_fields}\n",
        "\n",
        "Return only the corrected fields in JSON format.\n",
        "\"\"\"\n",
        "        try:\n",
        "            fixed_fields = llm2.invoke(repair_prompt).content.strip()\n",
        "            fixed_dict = json.loads(fixed_fields)\n",
        "            normalized_app.update({k.strip().upper(): v for k, v in fixed_dict.items()})\n",
        "        except Exception as e:\n",
        "            print(f\"Validation fix failed: {e}\")\n",
        "\n",
        "    # Apply updated structure back using a blank dictionary structure\n",
        "    canonical_app = {field: None for field in all_submission_fields}\n",
        "    for field in all_submission_fields:\n",
        "        matches = [k for k in normalized_app if k.replace(\"_\", \" \").upper().strip() == field.upper().strip()]\n",
        "        if matches:\n",
        "            canonical_app[field] = normalized_app[matches[0]]\n",
        "\n",
        "        # Normalize keys in GENERAL INFO QUESTIONS\n",
        "    if \"GENERAL INFO QUESTIONS\" in canonical_app and isinstance(canonical_app[\"GENERAL INFO QUESTIONS\"], dict):\n",
        "        normalized_questions = {}\n",
        "        for i, (k, v) in enumerate(canonical_app[\"GENERAL INFO QUESTIONS\"].items(), start=1):\n",
        "            normalized_questions[f\"Question {i}\"] = v\n",
        "        canonical_app[\"GENERAL INFO QUESTIONS\"] = normalized_questions\n",
        "\n",
        "    return canonical_app\n",
        "    prompt = f\"\"\"\n",
        "You are an insurance compliance analyst. You have received the following business owner policy (BOP) insurance application data:\n",
        "\n",
        "{json.dumps(flat_app, indent=2)}\n",
        "\n",
        "Evaluate this application for realism and internal consistency. Return a single word answer: VALID if the data looks realistic and consistent, or INVALID if it seems incomplete, inconsistent, or implausible.\n",
        "\"\"\"\n",
        "    try:\n",
        "        judgment = llm2.invoke(prompt).strip().upper()\n",
        "        return judgment.startswith(\"VALID\")\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# === Batch Simulation ===\n",
        "def batch_generate_applications(n_per_category=1):\n",
        "    try:\n",
        "        with open(\"empty_bop_template.json\") as f:\n",
        "            template_fields = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        create_empty_application_template()\n",
        "        with open(\"empty_bop_template.json\") as f:\n",
        "            template_fields = json.load(f)\n",
        "    applications = []\n",
        "    external_data = []\n",
        "    for category in bop_categories:\n",
        "        for i in range(n_per_category):\n",
        "            try:\n",
        "                business_type = f\"{category} - {uuid.uuid4().hex[:6]}\"\n",
        "                app = generate_application_incrementally(business_type)\n",
        "                app = validate_application(app)\n",
        "                credit = simulate_credit_score()\n",
        "                reviews = simulate_google_reviews(app.get(\"NAME\", business_type), business_type)\n",
        "                claims = simulate_claims_history(credit[\"score\"], reviews[\"average_rating\"])\n",
        "\n",
        "                applications.append(app)\n",
        "                external_data.append({\n",
        "                    \"Business Name\": app.get(\"NAME\", business_type),\n",
        "                    \"Credit Score\": credit[\"score\"],\n",
        "                    \"Credit Rating\": credit[\"rating\"],\n",
        "                    \"Google Review Count\": reviews[\"review_count\"],\n",
        "                    \"Average Review Rating\": reviews[\"average_rating\"],\n",
        "                    \"Sample Reviews\": reviews[\"reviews\"],\n",
        "                    \"Number of Claims\": claims[\"num_claims\"],\n",
        "                    \"Total Losses\": claims[\"total_losses\"]\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating for {category}: {e}\")\n",
        "\n",
        "    pd.DataFrame(applications).to_json(\"generated_bop_applications.json\", orient=\"records\", indent=2)\n",
        "    pd.DataFrame(external_data).to_json(\"generated_bop_third_party_data.json\", orient=\"records\", indent=2)\n",
        "    print(\"Batch generation completed.\")\n",
        "\n",
        "# === Incremental Application Generation ===\n",
        "def generate_application_incrementally(business_type: str) -> dict:\n",
        "    with get_openai_callback() as cb:\n",
        "        guidelines = get_guidelines(business_type)\n",
        "        questions = question_chain.invoke({\"business_type\": business_type, \"guidelines\": guidelines})[\"text\"]\n",
        "        answers = answer_chain.invoke({\"business_type\": business_type, \"questions\": questions})[\"text\"]\n",
        "        qa_pairs = \"\\n\".join(\n",
        "            f\"{q.strip()} — {a.strip()}\"\n",
        "            for q, a in zip(questions.strip().split(\"\\n\"), answers.strip().split(\"\\n\"))\n",
        "            if q.strip() and a.strip()\n",
        "        )\n",
        "\n",
        "        owner_description = owner_chain.invoke({\"business_type\": business_type, \"questions\": questions})[\"text\"]\n",
        "\n",
        "        application = {field: None for field in all_submission_fields}\n",
        "        context_base = f\"\"\"\n",
        "Business Type: {business_type}\n",
        "\n",
        "Owner Description:\n",
        "{owner_description}\n",
        "\n",
        "Answered Questions:\n",
        "{qa_pairs}\n",
        "\"\"\"\n",
        "\n",
        "        for field in all_submission_fields:\n",
        "            context = json.dumps({k: v for k, v in application.items() if v is not None}, indent=2)\n",
        "            prompt = f\"\"\"\n",
        "You are completing a Business Owner Policy (BOP) insurance application.\n",
        "\n",
        "Given the business details and previously answered fields below, generate the value for the field: \"{field}\".\n",
        "If you cannot confidently infer a value, provide a realistic placeholder.\n",
        "\n",
        "{context_base}\n",
        "\n",
        "Known Fields:\n",
        "{context}\n",
        "\n",
        "Return a single realistic value for \"{field}\" only.\n",
        "\"\"\"\n",
        "            try:\n",
        "                result = llm2.invoke(prompt).content.strip()\n",
        "                if result.lower().startswith(field.lower()):\n",
        "                    result = result.split(\":\", 1)[-1].strip()\n",
        "                application[field] = result\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating field {field}: {e}\")\n",
        "\n",
        "        print(f\"Total tokens used: {cb.total_tokens}\")\n",
        "        return application\n",
        "\n",
        "\n",
        "# === Main Function ===\n",
        "def generate_bop_application(business_type: str) -> dict:\n",
        "    with get_openai_callback() as cb:\n",
        "        guidelines = get_guidelines(business_type)\n",
        "\n",
        "        questions = question_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"guidelines\": guidelines\n",
        "        })[\"text\"]\n",
        "\n",
        "        answers = answer_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        qa_pairs = \"\\n\".join(\n",
        "            f\"{q.strip()} — {a.strip()}\"\n",
        "            for q, a in zip(questions.strip().split(\"\\n\"), answers.strip().split(\"\\n\"))\n",
        "            if q.strip() and a.strip()\n",
        "        )\n",
        "\n",
        "        owner_description = owner_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"questions\": questions\n",
        "        })[\"text\"]\n",
        "\n",
        "        final_app = application_chain.invoke({\n",
        "            \"business_type\": business_type,\n",
        "            \"qa_pairs\": qa_pairs,\n",
        "            \"fields_to_fill\": fields_to_fill,\n",
        "            \"business_owner_description\": owner_description\n",
        "        })[\"text\"]\n",
        "\n",
        "        print(f\"Total tokens used: {cb.total_tokens}\")\n",
        "        return json.loads(final_app)\n",
        "\n",
        "    create_empty_application_template()\n",
        "\n",
        "# === Example Run ===\n",
        "if __name__ == \"__main__\":\n",
        "    batch_generate_applications(n_per_category=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHWX9iDz32g0",
        "outputId": "86d4262b-4612-497b-b7d6-f1ef1fff876c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-db16885b46ad>:30: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
            "<ipython-input-13-db16885b46ad>:116: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  question_chain = LLMChain(llm=llm, prompt=question_prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty application template saved to empty_bop_template.json\n",
            "Total tokens used: 97385\n",
            "Total tokens used: 98899\n",
            "Total tokens used: 105540\n",
            "Total tokens used: 97743\n",
            "Batch generation completed.\n"
          ]
        }
      ]
    }
  ]
}